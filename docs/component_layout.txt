================================================================================
                    DUCKDB DATA DIFF - COMPONENT LAYOUT
                    Easy-to-Understand Architecture Guide
================================================================================
                            Version 3.0 (December 2024)
================================================================================

This document explains what each part of the codebase does in simple terms.
Think of it as a map of all the components and how they work together!

================================================================================
THE BIG PICTURE - How Everything Fits Together
================================================================================

    Your Files ï¿½ [File Reader] ï¿½ [Data Processor] ï¿½ [Comparator] ï¿½ Results
                       ï¿½              ï¿½                 ï¿½
                  [Progress Bar] [Validator]      [Report Generator]

The system works like an assembly line:
1. Reads your files (Excel, CSV, etc.)
2. Cleans and prepares the data
3. Compares the datasets
4. Generates easy-to-read reports

================================================================================
MAIN ENTRY POINTS
================================================================================

=ï¿½ main.py
----------
What it does: The main control center - starts everything
Think of it as: The conductor of an orchestra
Key features:
  " Loads your configuration file
  " Coordinates all the other components
  " Shows progress as things happen
  " Handles errors gracefully

=ï¿½ compare_datasets.py (NEW MENU INTERFACE)
-------------------------------------------
What it does: Interactive menu-driven interface
Think of it as: A friendly wizard that guides you
Key features:
  " Scans data/raw directory automatically
  " Shows numbered file lists with sizes
  " No need to type file paths
  " Generates configurations on the fly
  " Integrates with existing pipeline

================================================================================
CORE COMPONENTS (src/core/)
================================================================================

<ï¿½ comparator.py
----------------
What it does: The brain that actually compares your data
Think of it as: A detective finding differences
Key features:
  " Finds rows only in left dataset
  " Finds rows only in right dataset
  " Identifies values that changed
  " Calculates match statistics

=ï¿½ lineage.py
-------------
What it does: Keeps track of everything that happened to your data
Think of it as: A historian recording every step
Key features:
  " Records where data came from
  " Tracks all transformations
  " Creates an audit trail
  " Shows data flow visually

================================================================================
CONFIGURATION MANAGEMENT (src/config/)
================================================================================

ï¿½ manager.py
-------------
What it does: Reads and manages your configuration files
Think of it as: A settings manager
Key features:
  " Loads YAML configuration files
  " Validates settings are correct
  " Provides defaults for missing values
  " Makes configuration easy to access

================================================================================
DATA PIPELINE (src/pipeline/)
================================================================================

=ï¿½ stager.py
------------
What it does: Prepares data for comparison (staging)
Think of it as: A prep cook getting ingredients ready
Key features:
  " Converts Excel to efficient format
  " Normalizes column names
  " Applies data cleaning rules
  " Creates temporary working copies

 validators.py
----------------
What it does: Checks data quality before comparison
Think of it as: Quality control inspector
Key features:
  " Checks for missing values
  " Verifies data types are correct
  " Finds duplicate rows
  " Ensures key columns are unique

=' column_normalizer.py
-----------------------
What it does: Makes column names consistent
Think of it as: A translator making everyone speak the same language
Key features:
  " "Customer Name" becomes "customer_name"
  " Handles special characters
  " Resolves naming conflicts
  " Makes matching easier

=ï¿½ chunked_processor.py
-----------------------
What it does: Processes huge files without running out of memory
Think of it as: Eating an elephant one bite at a time
Key features:
  " Reads files in small chunks
  " Never loads entire file at once
  " Handles millions of rows easily
  " Keeps memory usage low

================================================================================
FILE ADAPTERS (src/adapters/)
================================================================================

=ï¿½ file_reader.py
-----------------
What it does: Reads different file types (Excel, CSV, Parquet)
Think of it as: A universal translator for files
Key features:
  " Handles Excel (.xlsx, .xls)
  " Reads CSV files
  " Supports Parquet (fast format)
  " Caches files for faster re-reading

================================================================================
USER INTERFACE (src/ui/)
================================================================================

=ï¿½ progress.py
--------------
What it does: Shows progress bars and status
Think of it as: A dashboard showing what's happening
Key features:
  " Simple text progress bars
  " Time estimates for completion
  " Current operation display
  " Works on all terminals

<ï¿½ rich_progress.py
-------------------
What it does: Beautiful, colorful progress display
Think of it as: A fancy dashboard with graphics
Key features:
  " Colored progress bars
  " Live updating display
  " Multiple progress tracks
  " Tables and panels for results

=ï¿½ menu.py
----------
What it does: Interactive menu interface for file selection
Think of it as: A friendly file chooser and comparison launcher
Key features:
  " Colored progress bars
  " Live updating display
  " Multiple progress tracks
  " Tables and panels for results

================================================================================
UTILITIES (src/utils/)
================================================================================

=ï¿½ logger.py
------------
What it does: Records everything that happens
Think of it as: A diary keeping notes
Key features:
  " Structured logging
  " Different severity levels
  " Helps with debugging
  " Creates audit trail

>ï¿½ normalizers.py
-----------------
What it does: Cleans and standardizes text data
Think of it as: A data janitor
Key features:
  " Removes extra spaces
  " Fixes special characters
  " Handles unicode issues
  " Standardizes formats

=ï¿½ converters.py
----------------
What it does: Converts between data types
Think of it as: A currency exchanger for data
Key features:
  " Converts "$1,234.56" to 1234.56
  " Handles True/False to t/f
  " Safely converts types
  " Handles errors gracefully

=ï¿½ metrics.py
-------------
What it does: Tracks performance and statistics
Think of it as: A fitness tracker for the pipeline
Key features:
  " Measures processing speed
  " Monitors memory usage
  " Tracks operation times
  " Generates performance reports

=ï¿½ recovery.py
--------------
What it does: Saves progress and allows resuming
Think of it as: A save game feature
Key features:
  " Creates checkpoints
  " Resumes from failures
  " Tracks completed steps
  " Prevents lost work

================================================================================
DATA FLOW - How Data Moves Through the System
================================================================================

Step 1: INPUT
-------------
Your Excel/CSV files ï¿½ file_reader.py ï¿½ Raw DataFrames

Step 2: STAGING
--------------
Raw DataFrames ï¿½ stager.py ï¿½ Cleaned, normalized data in Parquet

Step 3: VALIDATION
------------------
Staged data ï¿½ validators.py ï¿½ Quality checked and verified

Step 4: COMPARISON
------------------
Validated data ï¿½ comparator.py ï¿½ Differences identified

Step 5: OUTPUT
--------------
Comparison results ï¿½ Report generators ï¿½ Excel/CSV reports

================================================================================
HELPER COMPONENTS
================================================================================

=ï¿½ profile_dataset.py
---------------------
What it does: Analyzes datasets to understand their structure
Think of it as: A scout gathering intelligence
Key features:
  " Identifies data types
  " Finds patterns (emails, phones, etc.)
  " Suggests key columns
  " Calculates statistics

> smart_matcher.py
-------------------
What it does: Automatically matches columns between datasets
Think of it as: A matchmaker for columns
Key features:
  " Uses multiple matching strategies
  " Provides confidence scores
  " Handles different column names
  " Suggests best matches

<ï¿½ generate_config.py
--------------------
What it does: Creates configuration files automatically
Think of it as: A template generator
Key features:
  " Builds YAML configurations
  " Sets up column mappings
  " Configures comparisons
  " Adds appropriate settings

=ï¿½ generate_test_data.py
------------------------
What it does: Creates test datasets for development
Think of it as: A data factory for testing
Key features:
  " Generates datasets of any size
  " Creates matching pairs
  " Adds controlled differences
  " Tests performance limits

================================================================================
DIRECTORY STRUCTURE
================================================================================

=ï¿½ data/
--------
  raw/          Your input files go here
  reports/      Comparison results appear here
  cached/       Speeds up re-reading files
  staging/      Temporary files during processing
  .checkpoints/ Saves progress for resume

=ï¿½ config/
----------
  Where you store your configuration files
  
=ï¿½ src/
-------
  All the component code (organized by function)
  
=ï¿½ tests/
---------
  Automated tests to ensure everything works
  
=ï¿½ docs/
--------
  Documentation (like this file!)

================================================================================
HOW COMPONENTS WORK TOGETHER - A Real Example
================================================================================

MENU INTERFACE FLOW:
When you run: python compare_datasets.py

1. MenuInterface (ui/menu.py) scans data/raw directory
   ï¿½
2. Shows numbered list of available files
   ï¿½
3. User selects left and right datasets
   ï¿½
4. MenuInterface generates config automatically
   ï¿½
5. Launches DataDiffPipeline with temp config
   ï¿½
6. Pipeline runs as normal (see below)

COMMAND-LINE FLOW:
When you run: python main.py datasets.yaml

1. main.py starts and reads datasets.yaml
   ï¿½
2. ConfigManager (config/manager.py) loads and validates settings
   ï¿½
3. FileReader (adapters/file_reader.py) reads your Excel/CSV files
   ï¿½
4. DataStager (pipeline/stager.py) cleans and prepares data
   ï¿½
5. ColumnNormalizer (pipeline/column_normalizer.py) fixes column names
   ï¿½
6. ValidationPipeline (pipeline/validators.py) checks data quality
   ï¿½
7. DataComparator (core/comparator.py) finds differences
   ï¿½
8. RichProgressMonitor (ui/rich_progress.py) shows progress throughout
   ï¿½
9. Results saved to data/reports/ folder

If anything fails:
- RecoveryManager (utils/recovery.py) saves the state
- You can resume from where it stopped!

================================================================================
KEY DESIGN PRINCIPLES
================================================================================

<ï¿½ SINGLE RESPONSIBILITY
Each component does ONE thing well

=' MODULARITY
Components can be swapped or upgraded independently

=ï¿½ SIZE LIMITS
No file exceeds 400 lines (most under 300)

>ï¿½ TESTABILITY
Every component can be tested in isolation

=ï¿½ PERFORMANCE
Handles millions of rows in minutes, not hours

=ï¿½ MEMORY EFFICIENCY
Never loads entire dataset into memory at once

= RESUMABILITY
Can recover from failures and continue

=ï¿½ OBSERVABILITY
Complete logging and metrics for monitoring

================================================================================
SUMMARY - Why This Architecture?
================================================================================

The modular architecture makes the system:

 RELIABLE: Each piece is simple and well-tested
 SCALABLE: Handles tiny files to massive datasets
 MAINTAINABLE: Easy to fix or improve individual parts
 FLEXIBLE: Can add new features without breaking existing ones
 EFFICIENT: Optimized for both speed and memory usage
 USER-FRIENDLY: Clear separation of UI from logic

Think of it like LEGO blocks - each piece is simple, but together they
build something powerful!

================================================================================
                          End of Component Layout Guide
================================================================================